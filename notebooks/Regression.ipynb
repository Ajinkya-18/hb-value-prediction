{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ceb812-77f9-47f7-bb99-22caa368e009",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8d0b4-2182-4a72-a629-b0121261a44f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6e35f-fcb1-427b-b05d-669a53525f61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Combining Images from Eyes Defy Anemia Dataset with Hgb Project Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb86c600-75c5-4b59-83e2-c821b63d533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # --- Configuration ---\n",
    "# root_dir = r\"../data/Conjunctiva/Original/eyes_defy_anemia/dataset_anemia\"\n",
    "# output_dir = r\"../data/Conjunctiva/Regression/eyes_defy_anemia_original_images\"\n",
    "\n",
    "# # Skip image files with these substrings\n",
    "# skip_keywords = [\"forniceal\", \"forniceal_palpebral\", \"palpebral\"]\n",
    "\n",
    "# # Valid image extensions\n",
    "# valid_exts = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "# # Ensure output directory exists\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Loop over region folders\n",
    "# for region in [\"India\", \"Italy\"]:\n",
    "#     region_path = os.path.join(root_dir, region)\n",
    "#     if not os.path.isdir(region_path):\n",
    "#         continue\n",
    "\n",
    "#     for sub_folder in os.listdir(region_path):\n",
    "#         sub_folder_path = os.path.join(region_path, sub_folder)\n",
    "\n",
    "#         # Skip files like .csv or .xlsx in the main India/Italy folder\n",
    "#         if os.path.isfile(sub_folder_path):\n",
    "#             continue\n",
    "\n",
    "#         # Proceed if item is a folder\n",
    "#         if os.path.isdir(sub_folder_path):\n",
    "#             for filename in os.listdir(sub_folder_path):\n",
    "#                 lower_name = filename.lower()\n",
    "#                 if lower_name.endswith(valid_exts) and not any(keyword in lower_name for keyword in skip_keywords):\n",
    "#                     src = os.path.join(sub_folder_path, filename)\n",
    "\n",
    "#                     # Rename to {Sub_Folder}_{Region}.ext\n",
    "#                     ext = os.path.splitext(filename)[1]\n",
    "#                     new_filename = f\"{sub_folder}_{region}{ext}\"\n",
    "#                     dst = os.path.join(output_dir, new_filename)\n",
    "\n",
    "#                     # Ensure no name collision\n",
    "#                     if os.path.exists(dst):\n",
    "#                         print(f\"Skipped duplicate: {new_filename}\")\n",
    "#                         continue\n",
    "\n",
    "#                     shutil.copy2(src, dst)\n",
    "\n",
    "# print(f\"Renamed and extracted original eye images saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d5c73-0da8-46a9-b0d4-08639a9a0118",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Cropping Conjunctiva Regions from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7dee0f4-3161-43aa-9b36-d34d23a78fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# import albumentations as A\n",
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "# # --- Paths ---\n",
    "# TEST_DIR = \"../data/Conjunctiva/Regression/eyes_defy_anemia_original_images\"\n",
    "# MASKED_SAVE_DIR = \"../data/Conjunctiva/Regression/regression_dataset/Images\"\n",
    "# MODEL_PATH = \"../models/Segmentation/conjunctiva_segmentation_unet.pth\"\n",
    "\n",
    "# # Create directory to save cropped images\n",
    "# os.makedirs(CROPPED_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # --- Transforms ---\n",
    "# val_transform = A.Compose([\n",
    "#     A.Resize(512, 512),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                 std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# # --- Load Model ---\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # Use your custom UNet class if defined elsewhere\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet18\",\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=3,\n",
    "#     classes=1,  # Binary segmentation\n",
    "#     activation=None  # Output will be logits\n",
    "# )\n",
    "# model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- Process Images ---\n",
    "# image_files = [f for f in os.listdir(TEST_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# for image_name in tqdm(image_files, desc=\"Saving precise conjunctiva regions\"):\n",
    "#     image_path = os.path.join(TEST_DIR, image_name)\n",
    "\n",
    "#     # Load and preprocess image\n",
    "#     orig_img = cv2.imread(image_path)\n",
    "#     rgb_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "#     orig_h, orig_w = orig_img.shape[:2]\n",
    "\n",
    "#     resized = A.Resize(512, 512)(image=rgb_img)\n",
    "#     image_input = val_transform(image=resized[\"image\"])[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         pred = model(image_input)\n",
    "#         pred_mask = torch.sigmoid(pred).squeeze().cpu().numpy()\n",
    "#         binary_mask = (pred_mask > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "#     # Resize mask back to original resolution\n",
    "#     binary_mask = cv2.resize(binary_mask, (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#     # Find contours to create polygon mask\n",
    "#     contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     mask_canvas = np.zeros_like(orig_img, dtype=np.uint8)\n",
    "\n",
    "#     if contours:\n",
    "#         # Fill the mask with white where conjunctiva is\n",
    "#         cv2.drawContours(mask_canvas, contours, -1, (255, 255, 255), thickness=cv2.FILLED)\n",
    "\n",
    "#         # Bitwise AND to keep only conjunctiva region\n",
    "#         masked = cv2.bitwise_and(orig_img, mask_canvas)\n",
    "\n",
    "#         save_path = os.path.join(MASKED_SAVE_DIR, image_name)\n",
    "#         cv2.imwrite(save_path, masked)\n",
    "\n",
    "# print(f\"Masked conjunctiva regions saved to: {MASKED_SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "019f411a-8fc6-4222-804b-02451c4411f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conjunctiva Box Cropping - Train Set\n",
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# import albumentations as A\n",
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "# # --- Paths ---\n",
    "# TEST_DIR = \"../data/Conjunctiva/Regression/regression_dataset/Images\"\n",
    "# CROPPED_SAVE_DIR = \"../data/Conjunctiva/Regression/regression_dataset/Box_Conjunctiva_Images\"\n",
    "# MODEL_PATH = \"../models/Segmentation/conjunctiva_segmentation_unet.pth\"\n",
    "\n",
    "# # Create directory to save cropped images\n",
    "# os.makedirs(CROPPED_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # --- Transforms ---\n",
    "# val_transform = A.Compose([\n",
    "#     A.Resize(512, 512),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                 std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# # --- Load Model ---\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet18\",\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=3,\n",
    "#     classes=1,  # Binary segmentation\n",
    "#     activation=None  # Output will be logits\n",
    "# )\n",
    "# model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- Process Images ---\n",
    "# image_files = [f for f in os.listdir(TEST_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# for image_name in tqdm(image_files, desc=\"Cropping and saving Conjunctiva Boxes\"):\n",
    "#     image_path = os.path.join(TEST_DIR, image_name)\n",
    "\n",
    "#     # Load and preprocess image\n",
    "#     orig_img = cv2.imread(image_path)\n",
    "#     rgb_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "#     orig_h, orig_w = orig_img.shape[:2]\n",
    "\n",
    "#     resized = A.Resize(512, 512)(image=rgb_img)\n",
    "#     image_input = val_transform(image=resized[\"image\"])[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         pred = model(image_input)\n",
    "#         pred_mask = torch.sigmoid(pred).squeeze().cpu().numpy()\n",
    "#         binary_mask = (pred_mask > 0.7).astype(np.uint8) * 255\n",
    "\n",
    "#     # Resize mask back to original resolution\n",
    "#     binary_mask = cv2.resize(binary_mask, (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#     # Find contours and select the largest contour (box)\n",
    "#     contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     if contours:\n",
    "#         largest = max(contours, key=cv2.contourArea)\n",
    "#         x, y, w, h = cv2.boundingRect(largest)\n",
    "\n",
    "#         cropped_box = orig_img[y:y+h, x:x+w]\n",
    "        \n",
    "#         save_path = os.path.join(CROPPED_SAVE_DIR, image_name)\n",
    "#         cv2.imwrite(save_path, cropped_box)\n",
    "\n",
    "# print(f\"Masked conjunctiva regions saved to: {CROPPED_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb82918f-751c-4799-8fc4-01a724d54731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conjunctiva Box Cropping - Val set\n",
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# import albumentations as A\n",
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "# # --- Paths ---\n",
    "# TEST_DIR = \"../data/Conjunctiva/Regression/LMH_patient_data_mk2/HB Dataset\"\n",
    "# CROPPED_SAVE_DIR = \"../data/Conjunctiva/Regression/LMH_patient_data_mk2/HB_Cropped_Conjunctiva_Validation_Set\"\n",
    "# MODEL_PATH = \"../models/Segmentation/conjunctiva_segmentation_unet.pth\"\n",
    "\n",
    "# # Create directory to save cropped images\n",
    "# os.makedirs(CROPPED_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # --- Transforms ---\n",
    "# val_transform = A.Compose([\n",
    "#     A.Resize(512, 512),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                 std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# # --- Load Model ---\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet18\",\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=3,\n",
    "#     classes=1,  # Binary segmentation\n",
    "#     activation=None  # Output will be logits\n",
    "# )\n",
    "# model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- Process Images ---\n",
    "# image_files = [f for f in os.listdir(TEST_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# for image_name in tqdm(image_files, desc=\"Cropping + masking conjunctiva regions\"):\n",
    "#     image_path = os.path.join(TEST_DIR, image_name)\n",
    "\n",
    "#     # Load and preprocess image\n",
    "#     orig_img = cv2.imread(image_path)\n",
    "#     rgb_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "#     orig_h, orig_w = orig_img.shape[:2]\n",
    "\n",
    "#     resized = A.Resize(512, 512)(image=rgb_img)\n",
    "#     image_input = val_transform(image=resized[\"image\"])[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         pred = model(image_input)\n",
    "#         pred_mask = torch.sigmoid(pred).squeeze().cpu().numpy()\n",
    "#         binary_mask = (pred_mask > 0.7).astype(np.uint8) * 255\n",
    "\n",
    "#     # Resize mask back to original resolution\n",
    "#     binary_mask = cv2.resize(binary_mask, (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#     # Find largest contour\n",
    "#     contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     if contours:\n",
    "#         largest = max(contours, key=cv2.contourArea)\n",
    "#         x, y, w, h = cv2.boundingRect(largest)\n",
    "\n",
    "#         # Crop both the image and mask to the bounding box\n",
    "#         cropped_img = orig_img[y:y+h, x:x+w]\n",
    "#         cropped_mask = binary_mask[y:y+h, x:x+w]\n",
    "\n",
    "#         # Apply mask inside the crop\n",
    "#         mask_3d = cv2.merge([cropped_mask, cropped_mask, cropped_mask])\n",
    "#         masked_crop = cv2.bitwise_and(cropped_img, mask_3d)\n",
    "\n",
    "#         # Save final cropped+masked image\n",
    "#         save_path = os.path.join(CROPPED_SAVE_DIR, image_name)\n",
    "#         cv2.imwrite(save_path, masked_crop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2b2b6-50d6-4ee2-9402-81cc2ea66802",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Creating Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27732602-6bd9-4d74-a867-1693a506b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clg Hb dataset metadata correction\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Load metadata CSV\n",
    "# csv_path = \"../data/Conjunctiva/Regression/Patient's Data.xlsx\"\n",
    "# df = pd.read_excel(csv_path)\n",
    "\n",
    "# # Folder where images are stored\n",
    "# image_folder = \"../data/Conjunctiva/Regression/patient_data\"  # <-- change this to your actual folder path\n",
    "# image_files = os.listdir(image_folder)\n",
    "\n",
    "# # Build mapping from Id -> [image names without .jpg]\n",
    "# id_to_images = {}\n",
    "\n",
    "# for filename in image_files:\n",
    "#     if filename.endswith(\".jpg\"):\n",
    "#         basename = os.path.splitext(filename)[0]  # \"1vedant_righteye\"\n",
    "#         # Extract numeric Id from beginning of filename\n",
    "#         numeric_id = ''.join(filter(str.isdigit, basename.split('_')[0]))\n",
    "#         if numeric_id:\n",
    "#             numeric_id = int(numeric_id)\n",
    "#             id_to_images.setdefault(numeric_id, []).append(basename)\n",
    "\n",
    "# # Create new dataframe with duplicated and renamed rows\n",
    "# new_rows = []\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     id_val = row['ID']\n",
    "#     matched_images = id_to_images.get(id_val, [])\n",
    "#     if matched_images:\n",
    "#         for image_name in matched_images:\n",
    "#             new_row = row.copy()\n",
    "#             new_row['ID'] = image_name\n",
    "#             new_rows.append(new_row)\n",
    "\n",
    "# # Create updated DataFrame\n",
    "# updated_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# # Save to new CSV\n",
    "# updated_df.to_csv(\"../data/Conjunctiva/Regression/updated_metadata.csv\", index=False)\n",
    "# print(\"updated_metadata.csv has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd34d4df-aa4d-4eff-991a-fc3fdd83a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eyes defy anemia India Italy combined metadata\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # --- File paths ---\n",
    "# base_dir = r\"A:/AI-Projects/betic-projects/hb-project/data/Conjunctiva/Original/eyes_defy_anemia/dataset_anemia\"\n",
    "# india_csv_path = os.path.join(base_dir, \"India\", \"India.xlsx\")   # Adjust filename if needed\n",
    "# italy_csv_path = os.path.join(base_dir, \"Italy\", \"Italy.xlsx\")   # Adjust filename if needed\n",
    "# output_csv_path = '../data/Conjunctiva/Regression/regression_dataset/eyes_defy_anemia_combined_metadata.csv'\n",
    "\n",
    "# # --- Load the metadata ---\n",
    "# df_india = pd.read_excel(india_csv_path)\n",
    "# df_italy = pd.read_excel(italy_csv_path)\n",
    "\n",
    "# # --- Select required columns ---\n",
    "# columns_to_keep = ['Number', 'Hgb', 'Age', 'Gender']\n",
    "# df_india = df_india[columns_to_keep].copy()\n",
    "# df_italy = df_italy[columns_to_keep].copy()\n",
    "\n",
    "# # --- Update the 'Number' column to include dataset origin ---\n",
    "# df_india['Number'] = df_india['Number'].astype(str) + \"_India\"\n",
    "# df_italy['Number'] = df_italy['Number'].astype(str) + \"_Italy\"\n",
    "\n",
    "# # --- Combine the dataframes ---\n",
    "# df_combined = pd.concat([df_india, df_italy], ignore_index=True)\n",
    "\n",
    "# # --- Save the combined CSV ---\n",
    "# df_combined.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# print(f\"Combined metadata file saved at: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b82350b-ae6c-4d05-bbe7-c83e6d1c5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combined final metadata file.\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # --- File paths ---\n",
    "# existing_metadata_path = r\"../data/Conjunctiva/Regression/regression_dataset/eyes_defy_anemia_combined_metadata.csv\"\n",
    "# new_metadata_xlsx_path = r\"../data/Conjunctiva/Regression/updated_patients_data.csv\"  # Clg patients dataset\n",
    "# final_metadata_path = r\"../data/Conjunctiva/Regression/regression_dataset/final_metadata.csv\"\n",
    "\n",
    "# # --- Read existing metadata CSV ---\n",
    "# df_existing = pd.read_csv(existing_metadata_path)\n",
    "\n",
    "# # --- Read new metadata XLSX ---\n",
    "# df_new = pd.read_csv(new_metadata_xlsx_path)\n",
    "\n",
    "# # --- Extract and rename relevant columns ---\n",
    "# df_new = df_new[[\"ID\", \"AGE\", \"SEX\", \"HB\"]].copy()\n",
    "# df_new.rename(columns={\n",
    "#     \"ID\": \"Number\",\n",
    "#     \"AGE\": \"Age\",\n",
    "#     \"SEX\": \"Gender\",\n",
    "#     \"HB\": \"Hgb\"\n",
    "# }, inplace=True)\n",
    "\n",
    "# # --- Combine both datasets ---\n",
    "# df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "# # --- Save final metadata CSV ---\n",
    "# os.makedirs(os.path.dirname(final_metadata_path), exist_ok=True)\n",
    "# df_combined.to_csv(final_metadata_path, index=False)\n",
    "\n",
    "# print(f\"Final metadata file saved at: {final_metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0dfc37-0aaa-4ab6-9680-1a4e75171350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset Splitting into train valid and test sets\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "# import pandas as pd\n",
    "# import random\n",
    "\n",
    "# # --- Configurations ---\n",
    "# image_dir = r\"../data/Conjunctiva/Regression/regression_dataset/Images\"\n",
    "# metadata_path = r\"../data/Conjunctiva/Regression/regression_dataset/final_metadata.csv\"\n",
    "# output_root = r\"../data/Conjunctiva/Regression/reg_split_dataset\"\n",
    "\n",
    "# # Create destination folders\n",
    "# for split in [\"train\", \"valid\", \"test\"]:\n",
    "#     split_dir = os.path.join(output_root, split, \"images\")\n",
    "#     os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# # Read metadata\n",
    "# df = pd.read_csv(metadata_path)\n",
    "\n",
    "# # Shuffle the metadata\n",
    "# df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Split sizes\n",
    "# train_size = 300\n",
    "# valid_size = 100\n",
    "# test_size = 7\n",
    "\n",
    "# # Make the splits\n",
    "# df_train = df_shuffled.iloc[:train_size].copy()\n",
    "# df_valid = df_shuffled.iloc[train_size:train_size+valid_size].copy()\n",
    "# df_test  = df_shuffled.iloc[train_size+valid_size:train_size+valid_size+test_size].copy()\n",
    "\n",
    "# # Helper function to copy images to folders\n",
    "# def copy_images(split_df, split_name):\n",
    "#     for _, row in split_df.iterrows():\n",
    "#         img_name = row['Number']  # Use actual image filename\n",
    "#         src = os.path.join(image_dir, img_name)\n",
    "#         dst = os.path.join(output_root, split_name, \"images\", img_name)\n",
    "#         if os.path.exists(src):\n",
    "#             shutil.copy2(src, dst)\n",
    "#         else:\n",
    "#             print(f\"Image not found: {img_name}\")\n",
    "\n",
    "# # Copy images\n",
    "# copy_images(df_train, \"train\")\n",
    "# copy_images(df_valid, \"valid\")\n",
    "# copy_images(df_test, \"test\")\n",
    "\n",
    "# # Save metadata for each split\n",
    "# df_train.to_csv(os.path.join(output_root, \"train\", \"train_metadata.csv\"), index=False)\n",
    "# df_valid.to_csv(os.path.join(output_root, \"valid\", \"valid_metadata.csv\"), index=False)\n",
    "# df_test.to_csv(os.path.join(output_root, \"test\", \"test_metadata.csv\"), index=False)\n",
    "\n",
    "# print(\"Dataset split completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b55559-1034-43f2-babf-f5e843fee26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Improved script for dataset splitting\n",
    "# # Dataset Splitting into train valid and test sets (Only existing images)\n",
    "# import os\n",
    "# import shutil\n",
    "# import pandas as pd\n",
    "# import random\n",
    "\n",
    "# # --- Configurations ---\n",
    "# image_dir = r\"../data/Conjunctiva/Regression/regression_dataset/Images\"\n",
    "# metadata_path = r\"../data/Conjunctiva/Regression/regression_dataset/final_metadata.csv\"\n",
    "# output_root = r\"../data/Conjunctiva/Regression/reg_split_dataset\"\n",
    "\n",
    "# # Create destination folders\n",
    "# for split in [\"train\", \"valid\", \"test\"]:\n",
    "#     split_dir = os.path.join(output_root, split, \"images\")\n",
    "#     os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# # Read metadata\n",
    "# df = pd.read_csv(metadata_path)\n",
    "\n",
    "# # Filter metadata to only include existing images\n",
    "# def check_image_exists(img_name):\n",
    "#     # Try with .jpg extension\n",
    "#     if not img_name.endswith('.jpg'):\n",
    "#         img_name = img_name + '.jpg'\n",
    "#     return os.path.exists(os.path.join(image_dir, img_name))\n",
    "\n",
    "# # Filter dataframe to only include rows where images exist\n",
    "# df['image_exists'] = df['Number'].apply(check_image_exists)\n",
    "# df_existing = df[df['image_exists']].copy().drop('image_exists', axis=1)\n",
    "\n",
    "# print(f\"Total images in metadata: {len(df)}\")\n",
    "# print(f\"Existing images: {len(df_existing)}\")\n",
    "# print(f\"Missing images: {len(df) - len(df_existing)}\")\n",
    "\n",
    "# # Shuffle the existing metadata\n",
    "# df_shuffled = df_existing.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Adjust split sizes based on available images\n",
    "# total_existing = len(df_shuffled)\n",
    "# if total_existing < 407:  # 300 + 100 + 7\n",
    "#     print(f\"Warning: Only {total_existing} images available, adjusting split sizes proportionally...\")\n",
    "#     train_ratio = 300 / 407\n",
    "#     valid_ratio = 100 / 407\n",
    "#     test_ratio = 7 / 407\n",
    "    \n",
    "#     train_size = int(total_existing * train_ratio)\n",
    "#     valid_size = int(total_existing * valid_ratio)\n",
    "#     test_size = total_existing - train_size - valid_size\n",
    "# else:\n",
    "#     train_size = 300\n",
    "#     valid_size = 100\n",
    "#     test_size = 7\n",
    "\n",
    "# print(f\"Split sizes - Train: {train_size}, Valid: {valid_size}, Test: {test_size}\")\n",
    "\n",
    "# # Make the splits\n",
    "# df_train = df_shuffled.iloc[:train_size].copy()\n",
    "# df_valid = df_shuffled.iloc[train_size:train_size+valid_size].copy()\n",
    "# df_test  = df_shuffled.iloc[train_size+valid_size:train_size+valid_size+test_size].copy()\n",
    "\n",
    "# # Helper function to copy images to folders\n",
    "# def copy_images(split_df, split_name):\n",
    "#     copied_count = 0\n",
    "#     for _, row in split_df.iterrows():\n",
    "#         img_name = row['Number']\n",
    "        \n",
    "#         # Add .jpg extension if not present\n",
    "#         if not img_name.endswith('.jpg'):\n",
    "#             img_name = img_name + '.jpg'\n",
    "            \n",
    "#         src = os.path.join(image_dir, img_name)\n",
    "#         dst = os.path.join(output_root, split_name, \"images\", img_name)\n",
    "        \n",
    "#         if os.path.exists(src):\n",
    "#             shutil.copy2(src, dst)\n",
    "#             copied_count += 1\n",
    "#         else:\n",
    "#             print(f\"Unexpected: Image not found during copy: {img_name}\")\n",
    "    \n",
    "#     print(f\"Copied {copied_count} images to {split_name}\")\n",
    "#     return copied_count\n",
    "\n",
    "# # Copy images\n",
    "# print(\"\\nCopying images...\")\n",
    "# train_copied = copy_images(df_train, \"train\")\n",
    "# valid_copied = copy_images(df_valid, \"valid\")\n",
    "# test_copied = copy_images(df_test, \"test\")\n",
    "\n",
    "# # Save metadata for each split\n",
    "# df_train.to_csv(os.path.join(output_root, \"train\", \"train_metadata.csv\"), index=False)\n",
    "# df_valid.to_csv(os.path.join(output_root, \"valid\", \"valid_metadata.csv\"), index=False)\n",
    "# df_test.to_csv(os.path.join(output_root, \"test\", \"test_metadata.csv\"), index=False)\n",
    "\n",
    "# print(f\"\\nDataset split completed!\")\n",
    "# print(f\"Final counts - Train: {train_copied}, Valid: {valid_copied}, Test: {test_copied}\")\n",
    "# print(f\"Total images processed: {train_copied + valid_copied + test_copied}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4e69887e-2f09-4791-a4a2-98ae30aa0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # --- Paths ---\n",
    "# excel_file = \"../data/Conjunctiva/Regression/LMH_patient_data_mk2/PPG HB Data.xlsx\"   # Path to Excel sheet\n",
    "# images_dir = \"../data/Conjunctiva/Regression/LMH_patient_data_mk2/HB_Cropped_Conjunctiva_Validation_Set\"  # Folder containing images\n",
    "# output_csv = \"../data/Conjunctiva/Regression/LMH_patient_data_mk2/PPG_HB_Data_with_images.csv\"\n",
    "\n",
    "# # --- Load Excel sheet ---\n",
    "# df = pd.read_excel(excel_file)\n",
    "\n",
    "# # --- Build a map from ID -> image filenames ---\n",
    "# image_map = {}\n",
    "# for img in os.listdir(images_dir):\n",
    "#     if img.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "#         patient_id = img.split(\"_\", 1)[0]  # Extract ID\n",
    "#         if patient_id not in image_map:\n",
    "#             image_map[patient_id] = []\n",
    "#         image_map[patient_id].append(str(img).removesuffix('.jpg'))  # changed\n",
    "\n",
    "# # --- Build a new dataframe with one row per image ---\n",
    "# rows = []\n",
    "# for _, row in df.iterrows():\n",
    "#     patient_id = str(row[\"ID\"]).strip()\n",
    "#     if patient_id in image_map:\n",
    "#         for img_file in image_map[patient_id]:\n",
    "#             new_row = row.copy()\n",
    "#             new_row[\"Image_Files\"] = img_file\n",
    "#             rows.append(new_row)\n",
    "\n",
    "# # Create new dataframe\n",
    "# new_df = pd.DataFrame(rows)\n",
    "\n",
    "# # Set image filenames as index\n",
    "# new_df.set_index(\"Image_Files\", inplace=True)\n",
    "\n",
    "# # Save as CSV\n",
    "# new_df.to_csv(output_csv)\n",
    "# print(f\"CSV saved as: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cee28e-1da4-458f-80f2-6d1a165bb675",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f89536-11f0-4e54-b5b5-27cf5f89b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcdebb68-3a6e-433f-bed7-65ebacf0dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2LAB_CLAHE(object):\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # PIL -> array\n",
    "        img = np.array(img)\n",
    "        # RGB -> LAB\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        # Split channels\n",
    "        L, A, B = cv2.split(lab)\n",
    "        # Apply CLAHE only on L channel\n",
    "        L = self.clahe.apply(L)\n",
    "        # Merge back \n",
    "        lab = cv2.merge((L, A, B))\n",
    "        # Convert back to RGB (optional)\n",
    "        rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        # Ensuring valid uint8 range\n",
    "        rgb = np.clip(rgb, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return Image.fromarray(rgb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da09a83c-93e0-437f-be7e-d21b1386ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lab_mean_std(image_dir, clip_limit=2.0, tile_grid_size=(8,8)):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    means, stds = [], []\n",
    "\n",
    "    # loop through images\n",
    "    for img_name in tqdm(os.listdir(image_dir)):\n",
    "        if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # RGB -> LAB\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "        # CLAHE on L-channel\n",
    "        L, A, B = cv2.split(lab)\n",
    "        L = clahe.apply(L)\n",
    "        lab = cv2.merge((L, A, B))\n",
    "\n",
    "        # scale to [0,1] before computing mean/std\n",
    "        lab = lab.astype(np.float32) / 255.0\n",
    "\n",
    "        # per-channel mean/std\n",
    "        means.append(lab.reshape(-1, 3).mean(axis=0))\n",
    "        stds.append(lab.reshape(-1, 3).std(axis=0))\n",
    "\n",
    "    # average across dataset\n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c207c1e-d023-44b6-a0c5-f89cf2bc93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab_mean, lab_std = compute_lab_mean_std(image_dir='../data/Conjunctiva/Regression/regression_dataset/Box_Conjunctiva_Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822601a5-352e-4675-9231-3193270a9d47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e097de3-7f36-4e13-aea9-da329592bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        x = pred - target\n",
    "\n",
    "        x = x.clamp(min=-20, max=20)\n",
    "        loss = x + torch.nn.functional.softplus(-2.0 * x) - torch.log(torch.tensor(2.0, device=x.device))\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ae449c-09a7-463e-bda6-e35efe3579c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    RGB2LAB_CLAHE(), \n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.25)), \n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomRotation(15), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.289, 0.545, 0.505], std=[0.2864, 0.0456, 0.0205])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    RGB2LAB_CLAHE(), \n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.289, 0.545, 0.505], std=[0.2864, 0.0456, 0.0205])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5711ef7c-c4ae-4322-832e-fe70b62b28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        \n",
    "        if self.conv.bias is not None:\n",
    "            nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.conv(x)   # B, 1, H, W\n",
    "        attn = torch.sigmoid(attn)  # [0, 1]\n",
    "\n",
    "        return x * attn   # broadcast multiply\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbc990b-b6f9-4260-a9a2-c7f61f8bb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HgbCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HgbCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            ## First Convolutional Block\n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1, dilation=2, groups=3),\n",
    "            nn.GroupNorm(num_groups=1, num_channels=3), \n",
    "            nn.ReLU(),  \n",
    "            \n",
    "            # Second Convolutional Block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=2), \n",
    "            nn.GroupNorm(num_groups=8, num_channels=32),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third Convolutional Block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, groups=32), \n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Fourth Convolutional Block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, stride=1, padding=1, dilation=2), \n",
    "            nn.GroupNorm(num_groups=8, num_channels=128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Spatial Attention Block\n",
    "        self.attn = SpatialAttention(in_channels=128)\n",
    "            \n",
    "        # Classifier Head\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        # init\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.fc1(x), inplace=True)\n",
    "        x = nn.functional.relu(self.fc2(x), inplace=True)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aad13bd-0d30-4561-9ebe-5efefac35110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    print('===============================================Model Summary=============================================\\n')\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        status = 'Trainable' if param.requires_grad else 'Non-Trainable (Frozen)'\n",
    "        print(f'{name:30} | {status}')\n",
    "\n",
    "    print('\\n========================================================================================================\\n')\n",
    "\n",
    "    total_params = sum(param.numel() for param in model.parameters()) \n",
    "    trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(f'Total Parameters: {total_params:,}')\n",
    "    print(f'Trainable Parameters: {trainable_params:,}')\n",
    "    print(f'Frozen Parameters: {frozen_params:,}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1991b9a2-9d7f-41d4-9e7d-2b7f3ebc602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConjunctivaDataset(Dataset):\n",
    "    def __init__(self, image_dir, metadata_csv_path, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.metadata = pd.read_csv(metadata_csv_path)\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.valid_exts = ('.jpg', '.jpeg', '.png')\n",
    "        self.image_files = [img for img in os.listdir(self.image_dir) \n",
    "                            if img.lower().endswith(self.valid_exts) and os.path.splitext(img)[0] in list(self.metadata['Number'])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img_id = os.path.splitext(img_path)[0]\n",
    "        \n",
    "        image = Image.open(os.path.join(self.image_dir, img_path)).convert('RGB')\n",
    "        hb_val = float(self.metadata.loc[self.metadata['Number'] == img_id, 'Hgb'].values[0])\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        return image, hb_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638dbf19-ab6a-424e-96e3-98da5a4f9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ConjunctivaDataset(image_dir='../data/Conjunctiva/Regression/regression_dataset/Box_Conjunctiva_Images', \n",
    "                                   metadata_csv_path='../data/Conjunctiva/Regression/regression_dataset/regression_complete_metadata.csv', \n",
    "                                   transforms=train_transforms)\n",
    "\n",
    "val_dataset = ConjunctivaDataset(image_dir='../data/Conjunctiva/Regression/LMH_patient_data_mk2/HB_Cropped_Conjunctiva_Validation_Set', \n",
    "                                 metadata_csv_path='../data/Conjunctiva/Regression/LMH_patient_data_mk2/PPG_HB_Data_with_images.csv', \n",
    "                                 transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "323ba985-2307-4911-b9fd-618f1858cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_targets = train_dataset.metadata['Hgb'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb60714d-0565-4e23-a62a-93e7d2e9e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "bins = np.digitize(hb_targets, bins=np.linspace(min(hb_targets), max(hb_targets), 10))\n",
    "class_sample_count = np.bincount(bins)\n",
    "weights = 1. / class_sample_count[bins]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b0904b-93ff-4b91-a537-539d76aab152",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=24, sampler=sampler, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=24, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1ea84-3312-4ec6-8554-4fba8ab26637",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906da9cc-936a-412b-9a71-174cf66d27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, tol=0.01):\n",
    "        self.patience = patience\n",
    "        self.tol = tol\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.tol:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b34c3f1-83a4-4ea8-a877-e532856be7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_loader, val_loader, epochs=50, learning_rate=1e-1):\n",
    "    from tqdm import tqdm\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.5)\n",
    "    # loss_fn = LogCoshLoss()\n",
    "    # loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=3e-3, epochs=epochs, \n",
    "    #                                                 steps_per_epoch=len(train_loader), pct_start=0.1, \n",
    "    #                                                 div_factor=10, final_div_factor=100)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=f'../reports/{model_name}')\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        train_progress_bar = tqdm(train_loader, desc=f'Epoch: {epoch+1}/{epochs}', leave=True)\n",
    "\n",
    "        # image, hb_val => x, y\n",
    "        for x, y in train_progress_bar:\n",
    "            x, y = x.to(device), y.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "\n",
    "            train_progress_bar.set_postfix(loss=f'{loss.item():.3f}')\n",
    "\n",
    "        train_epoch_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_progress_bar = tqdm(val_loader, desc=f'Epoch: {epoch+1}/{epochs}', leave=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in val_progress_bar:\n",
    "                x, y = x.to(device), y.float().unsqueeze(1).to(device)\n",
    "                y_pred = model(x)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "                val_progress_bar.set_postfix(loss=f'{loss.item():.3f}')\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = val_epoch_loss\n",
    "            torch.save(model.state_dict(), f'../models/{model_name}_best_v1.pt')\n",
    "        \n",
    "        scheduler.step(val_epoch_loss)\n",
    "\n",
    "        writer.add_scalars('Loss', {'Train': train_epoch_loss, 'Val': val_epoch_loss}, epoch+1)\n",
    "        \n",
    "        print(f'Train Loss: {train_epoch_loss:.3f} | Val Loss: {val_epoch_loss:.3f}')\n",
    "        \n",
    "        early_stopping(val_epoch_loss)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print('Early Stopping triggered...')\n",
    "            print(f'Best Val Loss: {early_stopping.best_loss}')\n",
    "            break\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47baa5e-9670-48e3-add4-e14a29c9d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "hgbcnn = HgbCNN()\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "convnext = models.convnext_tiny(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18cd5d28-a6ff-4ca4-92e9-c0d46dd9f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features=256), \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(128, 64), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(64, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1f77494-ec48-4869-b1d1-f0f474c4b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in convnext.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "convnext.classifier[2] = nn.Sequential(\n",
    "    nn.Linear(768, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 128), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(128, 64), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac40b0f2-4567-4789-9e89-95bbaf54e002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================Model Summary=============================================\n",
      "\n",
      "features.0.0.weight            | Non-Trainable (Frozen)\n",
      "features.0.0.bias              | Non-Trainable (Frozen)\n",
      "features.0.1.weight            | Non-Trainable (Frozen)\n",
      "features.0.1.bias              | Non-Trainable (Frozen)\n",
      "features.1.0.layer_scale       | Non-Trainable (Frozen)\n",
      "features.1.0.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.1.0.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.1.0.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.1.0.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.1.0.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.1.0.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.1.0.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.1.0.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.1.1.layer_scale       | Non-Trainable (Frozen)\n",
      "features.1.1.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.1.1.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.1.1.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.1.1.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.1.1.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.1.1.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.1.1.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.1.1.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.1.2.layer_scale       | Non-Trainable (Frozen)\n",
      "features.1.2.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.1.2.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.1.2.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.1.2.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.1.2.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.1.2.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.1.2.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.1.2.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.2.0.weight            | Non-Trainable (Frozen)\n",
      "features.2.0.bias              | Non-Trainable (Frozen)\n",
      "features.2.1.weight            | Non-Trainable (Frozen)\n",
      "features.2.1.bias              | Non-Trainable (Frozen)\n",
      "features.3.0.layer_scale       | Non-Trainable (Frozen)\n",
      "features.3.0.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.3.0.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.3.0.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.3.0.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.3.0.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.3.0.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.3.0.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.3.0.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.3.1.layer_scale       | Non-Trainable (Frozen)\n",
      "features.3.1.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.3.1.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.3.1.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.3.1.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.3.1.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.3.1.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.3.1.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.3.1.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.3.2.layer_scale       | Non-Trainable (Frozen)\n",
      "features.3.2.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.3.2.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.3.2.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.3.2.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.3.2.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.3.2.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.3.2.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.3.2.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.4.0.weight            | Non-Trainable (Frozen)\n",
      "features.4.0.bias              | Non-Trainable (Frozen)\n",
      "features.4.1.weight            | Non-Trainable (Frozen)\n",
      "features.4.1.bias              | Non-Trainable (Frozen)\n",
      "features.5.0.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.0.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.0.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.0.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.0.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.0.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.0.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.0.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.0.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.1.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.1.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.1.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.1.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.1.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.1.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.1.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.1.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.1.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.2.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.2.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.2.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.2.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.2.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.2.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.2.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.2.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.2.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.3.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.3.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.3.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.3.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.3.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.3.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.3.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.3.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.3.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.4.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.4.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.4.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.4.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.4.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.4.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.4.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.4.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.4.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.5.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.5.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.5.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.5.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.5.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.5.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.5.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.5.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.5.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.6.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.6.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.6.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.6.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.6.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.6.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.6.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.6.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.6.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.7.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.7.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.7.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.7.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.7.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.7.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.7.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.7.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.7.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.5.8.layer_scale       | Non-Trainable (Frozen)\n",
      "features.5.8.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.5.8.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.5.8.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.5.8.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.5.8.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.5.8.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.5.8.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.5.8.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.6.0.weight            | Non-Trainable (Frozen)\n",
      "features.6.0.bias              | Non-Trainable (Frozen)\n",
      "features.6.1.weight            | Non-Trainable (Frozen)\n",
      "features.6.1.bias              | Non-Trainable (Frozen)\n",
      "features.7.0.layer_scale       | Non-Trainable (Frozen)\n",
      "features.7.0.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.7.0.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.7.0.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.7.0.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.7.0.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.7.0.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.7.0.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.7.0.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.7.1.layer_scale       | Non-Trainable (Frozen)\n",
      "features.7.1.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.7.1.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.7.1.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.7.1.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.7.1.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.7.1.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.7.1.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.7.1.block.5.bias      | Non-Trainable (Frozen)\n",
      "features.7.2.layer_scale       | Non-Trainable (Frozen)\n",
      "features.7.2.block.0.weight    | Non-Trainable (Frozen)\n",
      "features.7.2.block.0.bias      | Non-Trainable (Frozen)\n",
      "features.7.2.block.2.weight    | Non-Trainable (Frozen)\n",
      "features.7.2.block.2.bias      | Non-Trainable (Frozen)\n",
      "features.7.2.block.3.weight    | Non-Trainable (Frozen)\n",
      "features.7.2.block.3.bias      | Non-Trainable (Frozen)\n",
      "features.7.2.block.5.weight    | Non-Trainable (Frozen)\n",
      "features.7.2.block.5.bias      | Non-Trainable (Frozen)\n",
      "classifier.0.weight            | Non-Trainable (Frozen)\n",
      "classifier.0.bias              | Non-Trainable (Frozen)\n",
      "classifier.2.0.weight          | Trainable\n",
      "classifier.2.0.bias            | Trainable\n",
      "classifier.2.2.weight          | Trainable\n",
      "classifier.2.2.bias            | Trainable\n",
      "classifier.2.4.weight          | Trainable\n",
      "classifier.2.4.bias            | Trainable\n",
      "classifier.2.6.weight          | Trainable\n",
      "classifier.2.6.bias            | Trainable\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Total Parameters: 28,058,209\n",
      "Trainable Parameters: 238,081\n",
      "Frozen Parameters: 27,820,128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_summary(convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99e4276f-4c66-46b2-a942-b0ef09078e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================Model Summary=============================================\n",
      "\n",
      "conv1.weight                   | Non-Trainable (Frozen)\n",
      "bn1.weight                     | Non-Trainable (Frozen)\n",
      "bn1.bias                       | Non-Trainable (Frozen)\n",
      "layer1.0.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer1.0.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer1.0.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer1.0.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer1.0.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer1.0.bn2.bias              | Non-Trainable (Frozen)\n",
      "layer1.1.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer1.1.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer1.1.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer1.1.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer1.1.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer1.1.bn2.bias              | Non-Trainable (Frozen)\n",
      "layer2.0.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer2.0.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer2.0.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer2.0.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer2.0.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer2.0.bn2.bias              | Non-Trainable (Frozen)\n",
      "layer2.0.downsample.0.weight   | Non-Trainable (Frozen)\n",
      "layer2.0.downsample.1.weight   | Non-Trainable (Frozen)\n",
      "layer2.0.downsample.1.bias     | Non-Trainable (Frozen)\n",
      "layer2.1.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer2.1.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer2.1.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer2.1.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer2.1.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer2.1.bn2.bias              | Non-Trainable (Frozen)\n",
      "layer3.0.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer3.0.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer3.0.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer3.0.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer3.0.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer3.0.bn2.bias              | Non-Trainable (Frozen)\n",
      "layer3.0.downsample.0.weight   | Non-Trainable (Frozen)\n",
      "layer3.0.downsample.1.weight   | Non-Trainable (Frozen)\n",
      "layer3.0.downsample.1.bias     | Non-Trainable (Frozen)\n",
      "layer3.1.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer3.1.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer3.1.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer3.1.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer3.1.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer3.1.bn2.bias              | Non-Trainable (Frozen)\n",
      "layer4.0.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer4.0.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer4.0.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer4.0.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer4.0.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer4.0.bn2.bias              | Non-Trainable (Frozen)\n",
      "layer4.0.downsample.0.weight   | Non-Trainable (Frozen)\n",
      "layer4.0.downsample.1.weight   | Non-Trainable (Frozen)\n",
      "layer4.0.downsample.1.bias     | Non-Trainable (Frozen)\n",
      "layer4.1.conv1.weight          | Non-Trainable (Frozen)\n",
      "layer4.1.bn1.weight            | Non-Trainable (Frozen)\n",
      "layer4.1.bn1.bias              | Non-Trainable (Frozen)\n",
      "layer4.1.conv2.weight          | Non-Trainable (Frozen)\n",
      "layer4.1.bn2.weight            | Non-Trainable (Frozen)\n",
      "layer4.1.bn2.bias              | Non-Trainable (Frozen)\n",
      "fc.0.weight                    | Trainable\n",
      "fc.0.bias                      | Trainable\n",
      "fc.2.weight                    | Trainable\n",
      "fc.2.bias                      | Trainable\n",
      "fc.4.weight                    | Trainable\n",
      "fc.4.bias                      | Trainable\n",
      "fc.6.weight                    | Trainable\n",
      "fc.6.bias                      | Trainable\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Total Parameters: 11,349,057\n",
      "Trainable Parameters: 172,545\n",
      "Frozen Parameters: 11,176,512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_summary(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d644fabf-e30e-44fa-aaf2-33aafff19004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================Model Summary=============================================\n",
      "\n",
      "features.0.weight              | Trainable\n",
      "features.0.bias                | Trainable\n",
      "features.1.weight              | Trainable\n",
      "features.1.bias                | Trainable\n",
      "features.3.weight              | Trainable\n",
      "features.3.bias                | Trainable\n",
      "features.4.weight              | Trainable\n",
      "features.4.bias                | Trainable\n",
      "features.7.weight              | Trainable\n",
      "features.7.bias                | Trainable\n",
      "features.8.weight              | Trainable\n",
      "features.8.bias                | Trainable\n",
      "features.11.weight             | Trainable\n",
      "features.11.bias               | Trainable\n",
      "features.12.weight             | Trainable\n",
      "features.12.bias               | Trainable\n",
      "attn.conv.weight               | Trainable\n",
      "attn.conv.bias                 | Trainable\n",
      "fc1.weight                     | Trainable\n",
      "fc1.bias                       | Trainable\n",
      "fc2.weight                     | Trainable\n",
      "fc2.bias                       | Trainable\n",
      "fc3.weight                     | Trainable\n",
      "fc3.bias                       | Trainable\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Total Parameters: 35,302\n",
      "Trainable Parameters: 35,302\n",
      "Frozen Parameters: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_summary(hgbcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bd23fa0-9050-4682-a956-c5744f613f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|| 17/17 [00:47<00:00,  2.81s/it, loss=9.261]\n",
      "Epoch: 1/30: 100%|| 2/2 [00:00<00:00,  2.36it/s, loss=8.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 38.997 | Val Loss: 8.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30: 100%|| 17/17 [00:56<00:00,  3.32s/it, loss=1.740]\n",
      "Epoch: 2/30: 100%|| 2/2 [00:00<00:00,  2.56it/s, loss=1.736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.285 | Val Loss: 1.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30: 100%|| 17/17 [00:48<00:00,  2.83s/it, loss=1.471]\n",
      "Epoch: 3/30: 100%|| 2/2 [00:00<00:00,  2.44it/s, loss=1.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.924 | Val Loss: 1.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/30: 100%|| 17/17 [00:47<00:00,  2.81s/it, loss=1.942]\n",
      "Epoch: 4/30: 100%|| 2/2 [00:00<00:00,  2.54it/s, loss=1.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.018 | Val Loss: 1.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30: 100%|| 17/17 [00:47<00:00,  2.77s/it, loss=2.333]\n",
      "Epoch: 5/30: 100%|| 2/2 [00:00<00:00,  2.49it/s, loss=1.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.912 | Val Loss: 1.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30: 100%|| 17/17 [00:46<00:00,  2.74s/it, loss=1.737]\n",
      "Epoch: 6/30: 100%|| 2/2 [00:00<00:00,  2.37it/s, loss=1.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.002 | Val Loss: 1.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7/30: 100%|| 17/17 [00:48<00:00,  2.83s/it, loss=2.288]\n",
      "Epoch: 7/30: 100%|| 2/2 [00:00<00:00,  2.35it/s, loss=1.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.980 | Val Loss: 1.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30: 100%|| 17/17 [00:52<00:00,  3.08s/it, loss=1.594]\n",
      "Epoch: 8/30: 100%|| 2/2 [00:00<00:00,  2.39it/s, loss=1.340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.993 | Val Loss: 1.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30: 100%|| 17/17 [00:54<00:00,  3.22s/it, loss=2.023]\n",
      "Epoch: 9/30: 100%|| 2/2 [00:00<00:00,  2.14it/s, loss=1.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.990 | Val Loss: 1.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30: 100%|| 17/17 [00:56<00:00,  3.32s/it, loss=1.800]\n",
      "Epoch: 10/30: 100%|| 2/2 [00:00<00:00,  2.34it/s, loss=1.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.976 | Val Loss: 1.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11/30: 100%|| 17/17 [00:53<00:00,  3.12s/it, loss=2.196]\n",
      "Epoch: 11/30: 100%|| 2/2 [00:00<00:00,  2.05it/s, loss=1.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.883 | Val Loss: 1.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 12/30: 100%|| 17/17 [01:04<00:00,  3.80s/it, loss=2.318]\n",
      "Epoch: 12/30: 100%|| 2/2 [00:01<00:00,  1.65it/s, loss=1.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.999 | Val Loss: 1.280\n",
      "Early Stopping triggered...\n",
      "Best Val Loss: 1.2764017324189882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_model(hgbcnn, 'HgbCNN', train_loader, val_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c61ad5e9-8124-416a-b37b-62f0f8a0e57f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|| 17/17 [00:44<00:00,  2.63s/it, loss=11.319]\n",
      "Epoch: 1/30: 100%|| 2/2 [00:01<00:00,  1.30it/s, loss=11.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 562.754 | Val Loss: 10.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30: 100%|| 17/17 [00:47<00:00,  2.80s/it, loss=10.162]\n",
      "Epoch: 2/30: 100%|| 2/2 [00:01<00:00,  1.39it/s, loss=10.798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.158 | Val Loss: 10.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30: 100%|| 17/17 [00:42<00:00,  2.52s/it, loss=9.540]\n",
      "Epoch: 3/30: 100%|| 2/2 [00:01<00:00,  1.34it/s, loss=9.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.716 | Val Loss: 8.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/30: 100%|| 17/17 [00:41<00:00,  2.44s/it, loss=7.470]\n",
      "Epoch: 4/30: 100%|| 2/2 [00:01<00:00,  1.33it/s, loss=7.577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.701 | Val Loss: 7.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30: 100%|| 17/17 [00:47<00:00,  2.82s/it, loss=4.357]\n",
      "Epoch: 5/30: 100%|| 2/2 [00:01<00:00,  1.36it/s, loss=5.865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.137 | Val Loss: 5.420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30: 100%|| 17/17 [00:45<00:00,  2.66s/it, loss=4.539]\n",
      "Epoch: 6/30: 100%|| 2/2 [00:01<00:00,  1.39it/s, loss=4.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.463 | Val Loss: 3.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7/30: 100%|| 17/17 [00:43<00:00,  2.58s/it, loss=2.864]\n",
      "Epoch: 7/30: 100%|| 2/2 [00:01<00:00,  1.25it/s, loss=3.040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14.335 | Val Loss: 2.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30: 100%|| 17/17 [00:44<00:00,  2.63s/it, loss=2.537]\n",
      "Epoch: 8/30: 100%|| 2/2 [00:01<00:00,  1.36it/s, loss=2.190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.522 | Val Loss: 1.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30: 100%|| 17/17 [00:44<00:00,  2.60s/it, loss=2.179]\n",
      "Epoch: 9/30: 100%|| 2/2 [00:01<00:00,  1.35it/s, loss=2.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.056 | Val Loss: 2.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30: 100%|| 17/17 [00:47<00:00,  2.77s/it, loss=2.055]\n",
      "Epoch: 10/30: 100%|| 2/2 [00:01<00:00,  1.32it/s, loss=1.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.941 | Val Loss: 1.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11/30: 100%|| 17/17 [00:43<00:00,  2.55s/it, loss=1.952]\n",
      "Epoch: 11/30: 100%|| 2/2 [00:01<00:00,  1.34it/s, loss=1.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.908 | Val Loss: 1.278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 12/30: 100%|| 17/17 [00:43<00:00,  2.54s/it, loss=1.382]\n",
      "Epoch: 12/30: 100%|| 2/2 [00:01<00:00,  1.34it/s, loss=1.410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.713 | Val Loss: 1.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 13/30: 100%|| 17/17 [00:44<00:00,  2.63s/it, loss=1.834]\n",
      "Epoch: 13/30: 100%|| 2/2 [00:01<00:00,  1.29it/s, loss=1.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.859 | Val Loss: 1.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 14/30: 100%|| 17/17 [00:50<00:00,  2.97s/it, loss=1.782]\n",
      "Epoch: 14/30: 100%|| 2/2 [00:01<00:00,  1.39it/s, loss=1.410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.764 | Val Loss: 1.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 15/30: 100%|| 17/17 [00:47<00:00,  2.78s/it, loss=2.131]\n",
      "Epoch: 15/30: 100%|| 2/2 [00:01<00:00,  1.31it/s, loss=1.380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.891 | Val Loss: 1.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 16/30: 100%|| 17/17 [00:47<00:00,  2.79s/it, loss=1.679]\n",
      "Epoch: 16/30: 100%|| 2/2 [00:01<00:00,  1.35it/s, loss=1.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.784 | Val Loss: 1.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 17/30: 100%|| 17/17 [00:44<00:00,  2.60s/it, loss=2.182]\n",
      "Epoch: 17/30: 100%|| 2/2 [00:01<00:00,  1.31it/s, loss=1.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.975 | Val Loss: 1.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 18/30: 100%|| 17/17 [00:47<00:00,  2.80s/it, loss=1.882]\n",
      "Epoch: 18/30: 100%|| 2/2 [00:01<00:00,  1.32it/s, loss=1.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.852 | Val Loss: 1.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19/30: 100%|| 17/17 [00:44<00:00,  2.59s/it, loss=1.871]\n",
      "Epoch: 19/30: 100%|| 2/2 [00:01<00:00,  1.18it/s, loss=1.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.845 | Val Loss: 1.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20/30: 100%|| 17/17 [00:48<00:00,  2.83s/it, loss=2.196]\n",
      "Epoch: 20/30: 100%|| 2/2 [00:01<00:00,  1.34it/s, loss=1.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.946 | Val Loss: 1.268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 21/30: 100%|| 17/17 [00:43<00:00,  2.54s/it, loss=1.913]\n",
      "Epoch: 21/30: 100%|| 2/2 [00:01<00:00,  1.31it/s, loss=1.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.943 | Val Loss: 1.278\n",
      "Early Stopping triggered...\n",
      "Best Val Loss: 1.2692934564642004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_model(resnet18, 'ResNet18', train_loader, val_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e112ff55-edd7-4961-814c-d3a4069483bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30: 100%|| 17/17 [02:51<00:00, 10.08s/it, loss=5.469]\n",
      "Epoch: 1/30: 100%|| 2/2 [00:06<00:00,  3.16s/it, loss=4.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 176.617 | Val Loss: 4.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30: 100%|| 17/17 [01:47<00:00,  6.31s/it, loss=1.574]\n",
      "Epoch: 2/30: 100%|| 2/2 [00:06<00:00,  3.16s/it, loss=3.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.933 | Val Loss: 3.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30: 100%|| 17/17 [01:44<00:00,  6.17s/it, loss=1.844]\n",
      "Epoch: 3/30: 100%|| 2/2 [00:06<00:00,  3.15s/it, loss=3.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.279 | Val Loss: 3.258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/30: 100%|| 17/17 [01:46<00:00,  6.28s/it, loss=2.191]\n",
      "Epoch: 4/30: 100%|| 2/2 [00:06<00:00,  3.19s/it, loss=1.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.393 | Val Loss: 1.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30: 100%|| 17/17 [01:49<00:00,  6.44s/it, loss=1.826]\n",
      "Epoch: 5/30: 100%|| 2/2 [00:06<00:00,  3.36s/it, loss=1.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.602 | Val Loss: 1.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30: 100%|| 17/17 [01:54<00:00,  6.71s/it, loss=15.575]\n",
      "Epoch: 6/30: 100%|| 2/2 [00:06<00:00,  3.27s/it, loss=5.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.604 | Val Loss: 6.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7/30: 100%|| 17/17 [01:49<00:00,  6.44s/it, loss=8.126]\n",
      "Epoch: 7/30: 100%|| 2/2 [00:06<00:00,  3.21s/it, loss=8.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.682 | Val Loss: 7.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30: 100%|| 17/17 [01:54<00:00,  6.75s/it, loss=1.649]\n",
      "Epoch: 8/30: 100%|| 2/2 [00:06<00:00,  3.11s/it, loss=2.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.996 | Val Loss: 1.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30: 100%|| 17/17 [01:53<00:00,  6.69s/it, loss=1.757]\n",
      "Epoch: 9/30: 100%|| 2/2 [00:06<00:00,  3.14s/it, loss=1.968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.225 | Val Loss: 1.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30: 100%|| 17/17 [01:47<00:00,  6.34s/it, loss=0.883]\n",
      "Epoch: 10/30: 100%|| 2/2 [00:06<00:00,  3.19s/it, loss=1.832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.460 | Val Loss: 1.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11/30: 100%|| 17/17 [01:51<00:00,  6.56s/it, loss=1.357]\n",
      "Epoch: 11/30: 100%|| 2/2 [00:06<00:00,  3.40s/it, loss=1.747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.342 | Val Loss: 1.736\n",
      "Early Stopping triggered...\n",
      "Best Val Loss: 1.6403366359504494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_model(convnext, 'ConvNeXt', train_loader, val_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e39bf-674b-492a-a387-32a2815bd110",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfde870c-e365-4faf-9b65-72fc5eb72749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4e8c067-4b01-4cca-8eda-b33ccb8e9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensorboard_scalars(log_dir, tag='loss/train'):\n",
    "    ea = event_accumulator.EventAccumulator(log_dir)\n",
    "    ea.Reload()\n",
    "\n",
    "    return [x.value for x in ea.Scalars(tag)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aefc5072-3e1f-45a8-a360-49a87be941bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {\n",
    "    'HgbCNN': ('../reports/HgbCNN/Loss_Train', '../reports/HgbCNN/Loss_Val'), \n",
    "    'ResNet18': ('../reports/ResNet18/Loss_Train', '../reports/ResNet18/Loss_Val'), \n",
    "    'ConvNeXt': ('../reports/ConvNeXt/Loss_Train', '../reports/ConvNeXt/Loss_Val')\n",
    "}\n",
    "\n",
    "loss_curves = {}\n",
    "\n",
    "for model, path in runs.items():\n",
    "    train_loss = load_tensorboard_scalars(path[0], 'Loss')\n",
    "    val_loss = load_tensorboard_scalars(path[1], 'Loss')\n",
    "    loss_curves[model] = (train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7649134-cc5e-47c9-a1d4-fac9cd2e59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "\n",
    "# for model, (train, val) in loss_curves.items():\n",
    "#     plt.plot(train, label=f'{model} Train Loss', linestyle='--')\n",
    "#     plt.plot(val, label=f'{model} Val Loss')\n",
    "\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training vs Validation Loss (hgbCNN vs ResNet18 vs ConvNeXt')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../reports/HgbCNN_vs_ResNet18_vs_ConvNeXt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac7c3d23-7fb6-40b8-9d69-49d410f3c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model, (train, val) in loss_curves.items():\n",
    "#     plt.plot(train, label=f'{model} Train Loss', linestyle='--')\n",
    "#     plt.plot(val, label=f'{model} Val Loss')\n",
    "\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title(f'{model}: Training vs Validation Loss')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'../reports/{model}_Loss_Plot')\n",
    "#     plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b061be1-e5e7-4714-a0f3-2f58ad6b011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmd to view loss plots in tensorboard web server\n",
    "# tensorboard --logdir=../reports --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5eeba7-30ac-40f4-99ab-11dc5abc74c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
